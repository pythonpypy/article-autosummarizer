{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from heapq import nlargest\n",
    "from nltk.probability import FreqDist\n",
    "from collections import  defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from urllib.request import urlopen\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"input your washington post article url here\"\n",
    "url = \"https://www.washingtonpost.com/technology/2018/11/07/samsungs-next-phone-folds-up-like-book/?utm_term=.d48c1d9e13b9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Text_From_Washington_Post(url):\n",
    "    \n",
    "    \"\"\"Given a url the func returns the text from the article of washingtion post as a single string.\n",
    "       input -- >>> (url from washington post website)\n",
    "       output -- >>> (returns the text(str) from the article).\"\"\"\n",
    "    open_page = urlopen(url).read().decode(\"utf8\",\"ignore\")\n",
    "    soup = BeautifulSoup(open_page,\"lxml\")\n",
    "    # find all the tags that are there in the article(here article tag)\n",
    "    text = \" \".join(map(lambda s:s.text, soup.findAll(\"article\")))\n",
    "    # return the string version of the entire article with any encoding errors replaced with a space\n",
    "    return str(text.encode(encoding=\"ascii\",errors=\"replace\").replace(b\"?\",b\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Get_Text_From_Washington_Post(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(text, n):\n",
    "    \"\"\"Returns the n point summary of an article\n",
    "    inputs -- >>> text(article), n (number of summary sentences to return from the given article).\"\"\"\n",
    "    # break the text in to sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # go if number of points(Sentences) we want in the summary are less than the no of sentences in the text(original)\n",
    "    assert n <= len(sentences)\n",
    "    # break the text in to word tokens\n",
    "    words = word_tokenize(text.lower())\n",
    "    # create a stopwords list to filter out the stopwords from our tokenized words list\n",
    "    stopwords_list = set( stopwords.words(\"english\") + list(punctuation))\n",
    "    # words list with stopwords removed\n",
    "    words = [word for word in words if word not in stopwords_list]\n",
    "    # calculate how many times each word occurs\n",
    "    frequency =  FreqDist(words)\n",
    "    # rank the sentences based on the score of each sentence\n",
    "    #the sentence with most occured words have high score than others.\n",
    "    #default dict doesn't throw an error if a key is not present in the key lookup.\n",
    "    sentences_ranking = defaultdict(int)\n",
    "    # pass a sentence at a time\n",
    "    for i, sent in enumerate(sentences):\n",
    "        # tokenize the passed sentence in to words\n",
    "        for word in word_tokenize(sent.lower()):\n",
    "            #if word is present in frequency dict\n",
    "            if word in frequency:\n",
    "                # then take the word and append it the score it has in frequency dict\n",
    "                sentences_ranking[i] += frequency[word]\n",
    "        # gives the index of the sentences for summary\n",
    "    summary_sentences_index = nlargest(n, sentences_ranking,sentences_ranking.get)\n",
    "    # return the list of sentences in ascending order\n",
    "    return [sentences[index] for index in sorted(summary_sentences_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_summary(text, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
